# üìö Referencias Bibliogr√°ficas y Fuentes T√©cnicas

Este documento lista las fuentes acad√©micas, libros t√©cnicos y papers que fundamentan las t√©cnicas implementadas en este proyecto.

## üß† Natural Language Processing (NLP)

### Libros de Referencia

1. **Natural Language Processing with Python**
   - Autores: Steven Bird, Ewan Klein, Edward Loper
   - Editorial: O'Reilly Media
   - Uso: Fundamentos de NLP, tokenizaci√≥n, y procesamiento de corpus

2. **Practical Natural Language Processing**
   - Autores: Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, Harshit Surana
   - Editorial: O'Reilly Media
   - Uso: Implementaciones pr√°cticas de pipelines NLP

3. **Mastering Regular Expressions**
   - Autor: Jeffrey E.F. Friedl
   - Editorial: O'Reilly Media
   - Uso: T√©cnicas avanzadas de regex para parsing de texto

## ü§ñ Transformers y Modelos de Lenguaje

### Papers Acad√©micos

4. **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**
   - Autores: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
   - Conferencia: NAACL 2019
   - URL: https://arxiv.org/abs/1810.04805
   - Uso: Fundamentos de modelos BERT para embeddings y topic modeling

5. **Large Language Models Meet NLP: A Survey**
   - Autores: Varios
   - A√±o: 2023
   - Uso: Estado del arte en LLMs y sus aplicaciones en NLP

6. **SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems**
   - Autores: Alex Wang et al.
   - Conferencia: NeurIPS 2019
   - URL: https://arxiv.org/abs/1905.00537
   - Uso: Benchmarks para evaluaci√≥n de modelos de lenguaje

## üìä Topic Modeling

### Papers y Documentaci√≥n

7. **Evolution of Topic Modeling**
   - Tipo: Review Paper
   - Uso: Historia y evoluci√≥n de t√©cnicas de topic modeling (LDA ‚Üí BERTopic)

8. **Latent Dirichlet Allocation (LDA)**
   - Autores: David M. Blei, Andrew Y. Ng, Michael I. Jordan
   - Journal: JMLR 2003
   - URL: https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf
   - Uso: Algoritmo base para topic modeling probabil√≠stico

9. **BERTopic**
   - Autor: Maarten Grootendorst
   - Documentaci√≥n: https://maartengr.github.io/BERTopic/
   - GitHub: https://github.com/MaartenGr/BERTopic
   - Uso: Topic modeling con embeddings contextuales

## üéôÔ∏è Speech Recognition

### Modelos y Papers

10. **Whisper: Robust Speech Recognition via Large-Scale Weak Supervision**
    - Autores: Alec Radford, Jong Wook Kim, et al. (OpenAI)
    - A√±o: 2022
    - URL: https://arxiv.org/abs/2212.04356
    - GitHub: https://github.com/openai/whisper
    - Uso: Transcripci√≥n autom√°tica de audio

## üìñ Material de Curso

### Materiales Did√°cticos

11. **MNA - Maestr√≠a en Ciencia de Datos - NLP**
    - Instituci√≥n: [Nombre de Universidad]
    - Curso: Natural Language Processing
    - Semanas: 1, 2, 3, 4, 6, 8
    - Temas: Historia de NLP, Corpus, Tokenizaci√≥n, DTM/TF-IDF, LSI, Transformers
    - Uso: Base te√≥rica y fundamentos acad√©micos

## üîß Recursos T√©cnicos

12. **Python Regular Expressions - Cheat Sheet**
    - Tipo: Referencia r√°pida
    - Uso: Gu√≠a de consulta para expresiones regulares

13. **ArXiv Paper 101306.101310**
    - Uso: [Especificar tema relacionado con el proyecto]

## üìù C√≥mo usar estas referencias

### En C√≥digo
```python
# Ejemplo de cita en c√≥digo
def extract_topics_lda(self, corpus, num_topics=10):
    """
    Extract topics using Latent Dirichlet Allocation.
    
    Based on: Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003).
    Latent dirichlet allocation. JMLR, 3, 993-1022.
    """
```

### En Documentaci√≥n
Al documentar t√©cnicas implementadas, siempre incluir:
- **Qu√©**: Descripci√≥n de la t√©cnica
- **Por qu√©**: Justificaci√≥n de su uso
- **Fuente**: Referencia acad√©mica o paper

## ‚öñÔ∏è Nota sobre Derechos de Autor

- **NO se incluyen** los PDFs completos en el repositorio
- Se respetan los derechos de autor de todos los autores
- Se proporcionan **citas apropiadas** y enlaces a fuentes originales
- Para obtener los textos completos, consultar:
  - Bibliotecas universitarias
  - Plataformas acad√©micas (ArXiv, Google Scholar)
  - Editoriales oficiales (O'Reilly, IEEE, ACM)

## üîó Enlaces √ötiles

- **ArXiv**: https://arxiv.org/ (Papers de acceso abierto)
- **Google Scholar**: https://scholar.google.com/ (B√∫squeda acad√©mica)
- **Papers with Code**: https://paperswithcode.com/ (Papers + implementaciones)
- **Hugging Face**: https://huggingface.co/ (Modelos pre-entrenados)

---

**√öltima actualizaci√≥n**: Octubre 2025

**Mantenimiento**: Actualizar este archivo al agregar nuevas t√©cnicas o fuentes
